{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74fd9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d883133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147497fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = Namespace(graph_nodes=10, graph_edges=10, state_size=1, action_size=5, num_episodes=5, num_eval_episodes=3, epochs=2, log_dir='./src/experiments/smoke_train/logs', wandb_api_key=None, wandb_project=None, wandb_entity=None, wandb_run_name='smoke_train', wandb_resume=False, agent_configurations=[{'num_police_agents': 2, 'agent_money': 16}, {'num_police_agents': 2, 'agent_money': 18}, {'num_police_agents': 2, 'agent_money': 20}, {'num_police_agents': 2, 'agent_money': 22}, {'num_police_agents': 2, 'agent_money': 24}, {'num_police_agents': 2, 'agent_money': 20}, {'num_police_agents': 3, 'agent_money': 10}, {'num_police_agents': 3, 'agent_money': 12}, {'num_police_agents': 3, 'agent_money': 14}, {'num_police_agents': 3, 'agent_money': 16}, {'num_police_agents': 3, 'agent_money': 18}, {'num_police_agents': 4, 'agent_money': 10}, {'num_police_agents': 4, 'agent_money': 12}, {'num_police_agents': 4, 'agent_money': 14}, {'num_police_agents': 4, 'agent_money': 16}, {'num_police_agents': 5, 'agent_money': 8}, {'num_police_agents': 5, 'agent_money': 10}, {'num_police_agents': 5, 'agent_money': 12}, {'num_police_agents': 6, 'agent_money': 10}, {'num_police_agents': 6, 'agent_money': 6}, {'num_police_agents': 6, 'agent_money': 8}], random_seed=42, evaluate=False, config='./src/experiments/smoke_train/config.yml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb67e2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(graph_nodes=10\n",
      " graph_edges=10\n",
      " state_size=1\n",
      " action_size=5\n",
      " num_episodes=5\n",
      " num_eval_episodes=3\n",
      " epochs=2\n",
      " log_dir='./src/experiments/smoke_train/logs'\n",
      " wandb_api_key=None\n",
      " wandb_project=None\n",
      " wandb_entity=None\n",
      " wandb_run_name='smoke_train'\n",
      " wandb_resume=False\n",
      " agent_configurations=[{'num_police_agents': 2\n",
      " 'agent_money': 16}\n",
      " {'num_police_agents': 2\n",
      " 'agent_money': 18}\n",
      " {'num_police_agents': 2\n",
      " 'agent_money': 20}\n",
      " {'num_police_agents': 2\n",
      " 'agent_money': 22}\n",
      " {'num_police_agents': 2\n",
      " 'agent_money': 24}\n",
      " {'num_police_agents': 2\n",
      " 'agent_money': 20}\n",
      " {'num_police_agents': 3\n",
      " 'agent_money': 10}\n",
      " {'num_police_agents': 3\n",
      " 'agent_money': 12}\n",
      " {'num_police_agents': 3\n",
      " 'agent_money': 14}\n",
      " {'num_police_agents': 3\n",
      " 'agent_money': 16}\n",
      " {'num_police_agents': 3\n",
      " 'agent_money': 18}\n",
      " {'num_police_agents': 4\n",
      " 'agent_money': 10}\n",
      " {'num_police_agents': 4\n",
      " 'agent_money': 12}\n",
      " {'num_police_agents': 4\n",
      " 'agent_money': 14}\n",
      " {'num_police_agents': 4\n",
      " 'agent_money': 16}\n",
      " {'num_police_agents': 5\n",
      " 'agent_money': 8}\n",
      " {'num_police_agents': 5\n",
      " 'agent_money': 10}\n",
      " {'num_police_agents': 5\n",
      " 'agent_money': 12}\n",
      " {'num_police_agents': 6\n",
      " 'agent_money': 10}\n",
      " {'num_police_agents': 6\n",
      " 'agent_money': 6}\n",
      " {'num_police_agents': 6\n",
      " 'agent_money': 8}]\n",
      " random_seed=42\n",
      " evaluate=False\n",
      " config='./src/experiments/smoke_train/config.yml')\n"
     ]
    }
   ],
   "source": [
    "print(str(ns).replace(',', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730851c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d408c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c597136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from logger import Logger  # Your custom Logger class\n",
    "from RLAgent.gnn_agent import GNNAgent\n",
    "from Enviroment.yard import CustomEnvironment\n",
    "from torch_geometric.data import Data\n",
    "import random\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper, PettingZooEnv\n",
    "# Define the device at the beginning\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")  # You may consider logging this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a69da4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "# from pettingzoo.butterfly import pistonball_v6\n",
    "# kwargs = {\"n_pistons\": 21, \"continuous\": True}\n",
    "# env = PettingZooWrapper(env=pistonball_v6.parallel_env(**kwargs),return_state=True,\n",
    "#                         group_map=None, # Use default for parallel (all pistons grouped together)\n",
    "#                         )\n",
    "# # print(env.group_map)\n",
    "# env.reset()\n",
    "# # # env.rollout(10)\n",
    "\n",
    "# # # from pettingzoo.classic import tictactoe_v3\n",
    "# # # from torchrl.envs.libs.pettingzoo import PettingZooWrapper\n",
    "# # # from torchrl.envs.utils import MarlGroupMapType\n",
    "# # # env = PettingZooWrapper(\n",
    "# # #  env=tictactoe_v3.env(),\n",
    "# # #  use_mask=True, # Must use it since one player plays at a time\n",
    "# # #  group_map=None # # Use default for AEC (one group per player)\n",
    "# # #  )\n",
    "# # # print(env.group_map)\n",
    "# # # env.rollout(10)\n",
    "# print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "038b92fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:31:38,567 - INFO - DifficultyNet initialized and moved to device.\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(\n",
    "        log_dir=args.log_dir,\n",
    "        wandb_api_key=args.wandb_api_key,\n",
    "        wandb_project=args.wandb_project,\n",
    "        wandb_entity=args.wandb_entity,\n",
    "        wandb_run_name=args.wandb_run_name,\n",
    "        wandb_resume=args.wandb_resume\n",
    "    )\n",
    "\n",
    "logger.log(\"Logger initialized.\", level=\"debug\")\n",
    "\n",
    "# Initialize DifficultyNet and move it to the GPU\n",
    "reward_weight_net = RewardWeightNet().to(device)\n",
    "logger.log(\"DifficultyNet initialized and moved to device.\")\n",
    "\n",
    "optimizer = optim.Adam(reward_weight_net.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "logger.log(\"Loss function (MSELoss) initialized.\", level=\"debug\")\n",
    "\n",
    "logger.log(f\"Starting training with variable agents and money settings.\", level=\"debug\")\n",
    "\n",
    "# Validate that the agent configurations list is provided and not empty\n",
    "if not hasattr(args, 'agent_configurations') or not args.agent_configurations:\n",
    "    raise ValueError(\"args.agent_configurations must be a non-empty list of (num_agents, agent_money) tuples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4c20ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-09 19:31:43,818 - INFO - Starting epoch 1/2.\n",
      "2025-05-09 19:31:43,824 - INFO - Choosen configuration: 3 agents, 10 money.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num agents: 3,\n",
      " agent money: 10,\n",
      " reward weights: {'Police_distance': tensor(0.1503, grad_fn=<SelectBackward0>), 'Police_group': tensor(0.9522, grad_fn=<SelectBackward0>), 'Police_position': tensor(0.0183, grad_fn=<SelectBackward0>), 'Police_time': tensor(0.8456, grad_fn=<SelectBackward0>), 'Mrx_closest': tensor(0.9960, grad_fn=<SelectBackward0>), 'Mrx_average': tensor(0.0953, grad_fn=<SelectBackward0>), 'Mrx_position': tensor(0.6503, grad_fn=<SelectBackward0>), 'Mrx_time': tensor(0.6707, grad_fn=<SelectBackward0>)}\n",
      "graph nodes: 10,\n",
      " graph edges: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Elena\\Documents\\elte\\ci\\student-mechanism-design\\ci\\Lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:282: UserWarning: PettingZoo in TorchRL is tested using version == 1.24.3 , If you are using a different version and are experiencing compatibility issues,please raise an issue in the TorchRL github.\n",
      "  warnings.warn(\n",
      "2025-05-09 19:31:45,510 - INFO - Epoch 1, Episode 1 started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'dict'>\n",
      "{'adjacency_matrix': array([[0., 0., 1., 0., 0., 0., 1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [1., 0., 0., 0., 0., 0., 0., 0., 1., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
      "       [0., 0., 1., 1., 1., 1., 1., 1., 0., 0.],\n",
      "       [0., 0., 0., 0., 0., 0., 0., 1., 0., 0.]]), 'node_features': array([[0., 0., 0., 0.],\n",
      "       [1., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 1., 0.],\n",
      "       [0., 1., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 1.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.],\n",
      "       [0., 0., 0., 0.]]), 'edge_index': array([[8, 8, 8, 8, 8, 4, 7, 8, 2, 0],\n",
      "       [3, 6, 5, 4, 7, 1, 9, 2, 0, 6]], dtype=int32), 'edge_features': array([1, 2, 1, 3, 1, 2, 1, 2, 2, 2]), 'MrX_pos': np.int32(1), 'Polices_pos': [np.int32(3), np.int32(6)], 'Currency': [10, 10, 10]}\n",
      "\n",
      "PRINT VAL\n",
      "\n",
      "K:adjacency_matrix\n",
      "val:\n",
      "(10, 10)\n",
      "K:node_features\n",
      "val:\n",
      "(10, 4)\n",
      "K:edge_index\n",
      "val:\n",
      "(2, 10)\n",
      "K:edge_features\n",
      "val:\n",
      "(10,)\n",
      "K:MrX_pos\n",
      "val:\n",
      "()\n",
      "excepted!!\n",
      "K:Polices_pos\n",
      "val:\n",
      "[np.int32(3), np.int32(6)]\n",
      "excepted!!\n",
      "K:Currency\n",
      "val:\n",
      "[10, 10, 10]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of dict",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(args.num_episodes):\n\u001b[32m     59\u001b[39m     logger.log(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Episode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m started.\u001b[39m\u001b[33m\"\u001b[39m,level=\u001b[33m\"\u001b[39m\u001b[33minfo\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     state, _ = \u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepisode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Elena\\Documents\\elte\\ci\\student-mechanism-design\\ci\\Lib\\site-packages\\torchrl\\envs\\common.py:2788\u001b[39m, in \u001b[36mEnvBase.reset\u001b[39m\u001b[34m(self, tensordict, **kwargs)\u001b[39m\n\u001b[32m   2784\u001b[39m     tensordict_reset = \u001b[38;5;28mself\u001b[39m._reset(\n\u001b[32m   2785\u001b[39m         tensordict.select(*\u001b[38;5;28mself\u001b[39m.reset_keys, strict=\u001b[38;5;28;01mFalse\u001b[39;00m), **kwargs\n\u001b[32m   2786\u001b[39m     )\n\u001b[32m   2787\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2788\u001b[39m     tensordict_reset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_reset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensordict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2789\u001b[39m \u001b[38;5;66;03m# We assume that this is done properly\u001b[39;00m\n\u001b[32m   2790\u001b[39m \u001b[38;5;66;03m# if reset.device != self.device:\u001b[39;00m\n\u001b[32m   2791\u001b[39m \u001b[38;5;66;03m#     reset = reset.to(self.device, non_blocking=True)\u001b[39;00m\n\u001b[32m   2792\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m tensordict_reset \u001b[38;5;129;01mis\u001b[39;00m tensordict:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Elena\\Documents\\elte\\ci\\student-mechanism-design\\ci\\Lib\\site-packages\\torchrl\\envs\\libs\\pettingzoo.py:581\u001b[39m, in \u001b[36mPettingZooWrapper._reset\u001b[39m\u001b[34m(self, tensordict, **kwargs)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mtype\u001b[39m(observation_dict[agent]))\n\u001b[32m    578\u001b[39m \u001b[38;5;28mprint\u001b[39m(observation_dict[agent])\n\u001b[32m    579\u001b[39m group_observation[index] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mobservation_spec\u001b[49m\u001b[43m[\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mobservation\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[43m    \u001b[49m\u001b[43mindex\u001b[49m\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobservation_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43magent\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m group_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    583\u001b[39m     agent_info_dict = info_dict[agent]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Elena\\Documents\\elte\\ci\\student-mechanism-design\\ci\\Lib\\site-packages\\torchrl\\data\\tensor_specs.py:1960\u001b[39m, in \u001b[36mOneHot._encode_eager\u001b[39m\u001b[34m(self, val, space, ignore_device)\u001b[39m\n\u001b[32m   1958\u001b[39m         val = torch.as_tensor(val)\n\u001b[32m   1959\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1960\u001b[39m         val = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1962\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m space \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1963\u001b[39m     space = \u001b[38;5;28mself\u001b[39m.space\n",
      "\u001b[31mRuntimeError\u001b[39m: Could not infer dtype of dict"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    logger.log_scalar('epoch_step', epoch)\n",
    "\n",
    "    logger.log(f\"Starting epoch {epoch + 1}/{args.epochs}.\", level=\"info\")\n",
    "    \n",
    "    # Randomly select a (num_agents, agent_money) tuple from the predefined list\n",
    "    # print(args.agent_configurations)\n",
    "    selected_config = random.choice(args.agent_configurations)  # Ensure args.agent_configurations is defined\n",
    "    num_agents, agent_money = selected_config[\"num_police_agents\"], selected_config[\"agent_money\"]  # Unpack the tuple\n",
    "    logger.log(f\"Choosen configuration: {num_agents} agents, {agent_money} money.\", level=\"info\")\n",
    "    # print(selected_config)\n",
    "    logger.log_scalar('epoch/num_agents', num_agents)\n",
    "    logger.log_scalar('epoch/agent_money', agent_money)\n",
    "    # Predict the difficulty from the number of agents and money\n",
    "    inputs = torch.FloatTensor([[num_agents, agent_money, args.graph_nodes, args.graph_edges]]).to(device)  # Move inputs to GPU\n",
    "    predicted_weight = reward_weight_net(inputs)\n",
    "    reward_weights = {\n",
    "        \"Police_distance\" : predicted_weight[0,0],\n",
    "        \"Police_group\": predicted_weight[0,1],\n",
    "        \"Police_position\": predicted_weight[0,2],\n",
    "        \"Police_time\": predicted_weight[0,3],\n",
    "        \"Mrx_closest\": predicted_weight[0,4],\n",
    "        \"Mrx_average\": predicted_weight[0,5],\n",
    "        \"Mrx_position\": predicted_weight[0,6],\n",
    "        \"Mrx_time\": predicted_weight[0,7]\n",
    "    }\n",
    "\n",
    "    logger.log(f\"Epoch {epoch + 1}: Predicted weights: {reward_weights}\", level=\"debug\")\n",
    "    logger.log_weights(reward_weights)\n",
    "    # Create environment with predicted difficulty\n",
    "    env_base = CustomEnvironment(\n",
    "        number_of_agents=num_agents,\n",
    "        agent_money=agent_money,\n",
    "        reward_weights=reward_weights,\n",
    "        logger=logger,\n",
    "        epoch=epoch,\n",
    "        graph_nodes=args.graph_nodes,\n",
    "        graph_edges=args.graph_edges\n",
    "    )\n",
    "    print(f\"num agents: {num_agents},\\n agent money: {agent_money},\\n reward weights: {reward_weights}\")\n",
    "    print(f\"graph nodes: {args.graph_nodes},\\n graph edges: {args.graph_edges}\")\n",
    "    env = PettingZooWrapper(env=env_base)\n",
    "    # env = PettingZooEnv(env=env_base, parallel=True)\n",
    "    logger.log(f\"Environment created with weights {reward_weights}.\",level=\"debug\")\n",
    "\n",
    "    # Determine node feature size from the environment\n",
    "    node_feature_size = env.number_of_agents + 1  # Assuming node features exist\n",
    "    mrX_action_size = env.action_space('MrX').n\n",
    "    police_action_size = env.action_space('Police0').n  # Assuming all police have the same action space\n",
    "    logger.log(f\"Node feature size: {node_feature_size}, MrX action size: {mrX_action_size}, Police action size: {police_action_size}\",level=\"debug\")\n",
    "\n",
    "    # Initialize GNN agents with graph-specific parameters and move them to GPU\n",
    "    mrX_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "    police_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "    logger.log(\"GNN agents for MrX and Police initialized.\",level=\"debug\")\n",
    "\n",
    "    # Train the MrX and Police agents in the environment\n",
    "    for episode in range(args.num_episodes):\n",
    "        logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1} started.\",level=\"info\")\n",
    "        state, _ = env.reset(episode=episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ee3819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pettingzoo import ParallelEnv\n",
    "\n",
    "# par_env = ParallelEnv()\n",
    "# par_env.possible_agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342636ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = Logger(\n",
    "#         log_dir=args.log_dir,\n",
    "#         wandb_api_key=args.wandb_api_key,\n",
    "#         wandb_project=args.wandb_project,\n",
    "#         wandb_entity=args.wandb_entity,\n",
    "#         wandb_run_name=args.wandb_run_name,\n",
    "#         wandb_resume=args.wandb_resume\n",
    "#     )\n",
    "\n",
    "# logger.log(\"Logger initialized.\", level=\"debug\")\n",
    "\n",
    "# # Initialize DifficultyNet and move it to the GPU\n",
    "# reward_weight_net = RewardWeightNet().to(device)\n",
    "# logger.log(\"DifficultyNet initialized and moved to device.\")\n",
    "\n",
    "# optimizer = optim.Adam(reward_weight_net.parameters(), lr=0.001)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# logger.log(\"Loss function (MSELoss) initialized.\", level=\"debug\")\n",
    "\n",
    "# logger.log(f\"Starting training with variable agents and money settings.\", level=\"debug\")\n",
    "\n",
    "# # Validate that the agent configurations list is provided and not empty\n",
    "# if not hasattr(args, 'agent_configurations') or not args.agent_configurations:\n",
    "#     raise ValueError(\"args.agent_configurations must be a non-empty list of (num_agents, agent_money) tuples.\")\n",
    "\n",
    "# for epoch in range(args.epochs):\n",
    "#     logger.log_scalar('epoch_step', epoch)\n",
    "\n",
    "#     logger.log(f\"Starting epoch {epoch + 1}/{args.epochs}.\", level=\"info\")\n",
    "    \n",
    "#     # Randomly select a (num_agents, agent_money) tuple from the predefined list\n",
    "#     # print(args.agent_configurations)\n",
    "#     selected_config = random.choice(args.agent_configurations)  # Ensure args.agent_configurations is defined\n",
    "#     num_agents, agent_money = selected_config[\"num_police_agents\"], selected_config[\"agent_money\"]  # Unpack the tuple\n",
    "#     logger.log(f\"Choosen configuration: {num_agents} agents, {agent_money} money.\", level=\"info\")\n",
    "#     # print(selected_config)\n",
    "#     logger.log_scalar('epoch/num_agents', num_agents)\n",
    "#     logger.log_scalar('epoch/agent_money', agent_money)\n",
    "#     # Predict the difficulty from the number of agents and money\n",
    "#     inputs = torch.FloatTensor([[num_agents, agent_money, args.graph_nodes, args.graph_edges]]).to(device)  # Move inputs to GPU\n",
    "#     predicted_weight = reward_weight_net(inputs)\n",
    "#     reward_weights = {\n",
    "#         \"Police_distance\" : predicted_weight[0,0],\n",
    "#         \"Police_group\": predicted_weight[0,1],\n",
    "#         \"Police_position\": predicted_weight[0,2],\n",
    "#         \"Police_time\": predicted_weight[0,3],\n",
    "#         \"Mrx_closest\": predicted_weight[0,4],\n",
    "#         \"Mrx_average\": predicted_weight[0,5],\n",
    "#         \"Mrx_position\": predicted_weight[0,6],\n",
    "#         \"Mrx_time\": predicted_weight[0,7]\n",
    "#     }\n",
    "\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Predicted weights: {reward_weights}\", level=\"debug\")\n",
    "#     logger.log_weights(reward_weights)\n",
    "#     # Create environment with predicted difficulty\n",
    "#     env_base = CustomEnvironment(\n",
    "#         number_of_agents=num_agents,\n",
    "#         agent_money=agent_money,\n",
    "#         reward_weights=reward_weights,\n",
    "#         logger=logger,\n",
    "#         epoch=epoch,\n",
    "#         graph_nodes=args.graph_nodes,\n",
    "#         graph_edges=args.graph_edges\n",
    "#     )\n",
    "#     env = PettingZooWrapper(env=env_base)\n",
    "#     logger.log(f\"Environment created with weights {reward_weights}.\",level=\"debug\")\n",
    "\n",
    "#     # Determine node feature size from the environment\n",
    "#     node_feature_size = env.number_of_agents + 1  # Assuming node features exist\n",
    "#     mrX_action_size = env.action_space('MrX').n\n",
    "#     police_action_size = env.action_space('Police0').n  # Assuming all police have the same action space\n",
    "#     logger.log(f\"Node feature size: {node_feature_size}, MrX action size: {mrX_action_size}, Police action size: {police_action_size}\",level=\"debug\")\n",
    "\n",
    "#     # Initialize GNN agents with graph-specific parameters and move them to GPU\n",
    "#     mrX_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "#     police_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "#     logger.log(\"GNN agents for MrX and Police initialized.\",level=\"debug\")\n",
    "\n",
    "#     # Train the MrX and Police agents in the environment\n",
    "#     for episode in range(args.num_episodes):\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1} started.\",level=\"info\")\n",
    "#         state, _ = env.reset(episode=episode)\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             # Create graph data for GNN and move to GPU\n",
    "#             mrX_graph = create_graph_data(state, 'MrX', env).to(device)\n",
    "#             police_graphs = [\n",
    "#                 create_graph_data(state, f'Police{i}', env).to(device)\n",
    "#                 for i in range(num_agents)\n",
    "#             ]\n",
    "#             logger.log(f\"Created graph data for MrX and Police agents.\",level=\"debug\")\n",
    "\n",
    "#             # MrX selects an action\n",
    "#             # mrX_action = mrX_agent.select_action(mrX_graph, torch.ones(mrX_action_size, device=device))\n",
    "\n",
    "#             mrX_action_size = env.action_space('MrX').n\n",
    "#             mrX_possible_moves = env.get_possible_moves(0)\n",
    "#             action_mask = torch.zeros(mrX_graph.num_nodes, dtype=torch.int32, device=device)\n",
    "#             action_mask[ mrX_possible_moves] = 1\n",
    "#             mrX_action = mrX_agent.select_action(mrX_graph,action_mask)\n",
    "#             logger.log(f\"MrX selected action: {mrX_action}\",level=\"debug\")\n",
    "\n",
    "#             # Police agents select actions\n",
    "#             agent_actions = {'MrX': mrX_action}\n",
    "#             for i in range(num_agents):\n",
    "#                 police_action_size = env.action_space(f'Police{i}').n\n",
    "#                 police_possible_moves = env.get_possible_moves(i+1)\n",
    "#                 action_mask = torch.zeros(police_graphs[i].num_nodes, dtype=torch.int32, device=device)\n",
    "#                 action_mask[ police_possible_moves] = 1\n",
    "#                 police_action = police_agent.select_action(\n",
    "#                     police_graphs[i],\n",
    "#                     action_mask\n",
    "#                 )\n",
    "#                 agent_actions[f'Police{i}'] = police_action\n",
    "#                 logger.log(f\"Police{i} selected action: {police_action}\",level=\"debug\")\n",
    "\n",
    "#             # Execute actions for MrX and Police\n",
    "#             next_state, rewards, terminations, truncation, _, _ = env.step(agent_actions)\n",
    "#             logger.log(f\"Executed actions. Rewards: {rewards}, Terminations: {terminations}, Truncations: {truncation}\",level=\"debug\")\n",
    "\n",
    "#             done = terminations.get('Police0', False) or all(truncation.values())\n",
    "#             logger.log(f\"Episode done: {done}\",level=\"debug\")\n",
    "\n",
    "#             # Update MrX agent\n",
    "#             mrX_next_graph = create_graph_data(next_state, 'MrX', env).to(device)\n",
    "#             mrX_agent.update(\n",
    "#                 mrX_graph,\n",
    "#                 mrX_action,\n",
    "#                 rewards.get('MrX', 0.0),\n",
    "#                 mrX_next_graph,\n",
    "#                 not terminations.get('Police0', False)\n",
    "#             )\n",
    "#             logger.log(f\"MrX agent updated with reward: {rewards.get('MrX', 0.0)}\",level=\"debug\")\n",
    "\n",
    "#             # Update shared police agent\n",
    "#             for i in range(num_agents):\n",
    "#                 police_next_graph = create_graph_data(next_state, f'Police{i}', env).to(device)\n",
    "#                 police_agent.update(\n",
    "#                     police_graphs[i],\n",
    "#                     agent_actions.get(f'Police{i}'),\n",
    "#                     rewards.get(f'Police{i}', 0.0),\n",
    "#                     police_next_graph,\n",
    "#                     terminations.get(f'Police{i}', False)\n",
    "#                 )\n",
    "#                 logger.log(f\"Police{i} agent updated with reward: {rewards.get(f'Police{i}', 0.0)}\",level=\"debug\")\n",
    "\n",
    "#             total_reward += rewards.get('MrX', 0.0)\n",
    "#             state = next_state\n",
    "#             logger.log(f\"Total reward updated to: {total_reward}\",level=\"debug\")\n",
    "\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1}, Total Reward: {total_reward}\",level=\"debug\")\n",
    "#         # logger.log_scalar(f'Episode_total_reward{epoch}', total_reward, episode)\n",
    "\n",
    "#     # Evaluate performance and calculate the target difficulty\n",
    "#     logger.log(f\"Evaluating agent balance after epoch {epoch + 1}.\",level=\"debug\")\n",
    "#     # logger.log_model(mrX_agent, 'MrX')\n",
    "#     # logger.log_model(police_agent, 'Police')\n",
    "#     # logger.log_model(reward_weight_net, 'RewardWeightNet')\n",
    "\n",
    "#     wins = 0\n",
    "\n",
    "#     for episode in range(args.num_eval_episodes):\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Evaluation Episode {episode + 1} started.\",level=\"info\")\n",
    "#         state, _ = env.reset(episode=episode)\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             # Create graph data for GNN and move to GPU\n",
    "#             mrX_graph = create_graph_data(state, 'MrX', env).to(device)\n",
    "#             police_graphs = [\n",
    "#                 create_graph_data(state, f'Police{i}', env).to(device)\n",
    "#                 for i in range(num_agents)\n",
    "#             ]\n",
    "#             logger.log(f\"Created graph data for MrX and Police agents.\",level=\"debug\")\n",
    "\n",
    "#             # MrX selects an action\n",
    "#             # mrX_action = mrX_agent.select_action(mrX_graph, torch.ones(mrX_action_size, device=device))\n",
    "\n",
    "#             mrX_action_size = env.action_space('MrX').n\n",
    "#             mrX_possible_moves = env.get_possible_moves(0)\n",
    "#             action_mask = torch.zeros(mrX_graph.num_nodes, dtype=torch.int32, device=device)\n",
    "#             action_mask[ mrX_possible_moves] = 1\n",
    "#             mrX_action = mrX_agent.select_action(mrX_graph,action_mask)\n",
    "#             logger.log(f\"MrX selected action: {mrX_action}\",level=\"debug\")\n",
    "\n",
    "#             # Police agents select actions\n",
    "#             agent_actions = {'MrX': mrX_action}\n",
    "#             for i in range(num_agents):\n",
    "#                 police_action_size = env.action_space(f'Police{i}').n\n",
    "#                 police_possible_moves = env.get_possible_moves(i+1)\n",
    "#                 action_mask = torch.zeros(police_graphs[i].num_nodes, dtype=torch.int32, device=device)\n",
    "#                 action_mask[ police_possible_moves] = 1\n",
    "#                 police_action = police_agent.select_action(\n",
    "#                     police_graphs[i],\n",
    "#                     action_mask\n",
    "#                 )\n",
    "#                 agent_actions[f'Police{i}'] = police_action\n",
    "#                 logger.log(f\"Police{i} selected action: {police_action}\",level=\"debug\")\n",
    "\n",
    "#             # Execute actions for MrX and Police\n",
    "#             next_state, rewards, terminations, truncation, winner, _ = env.step(agent_actions)\n",
    "#             logger.log(f\"Executed actions. Rewards: {rewards}, Terminations: {terminations}, Truncations: {truncation}\",level=\"debug\")\n",
    "\n",
    "#             done = terminations.get('Police0', False) or all(truncation.values())\n",
    "#             logger.log(f\"Episode done: {done}\",level=\"debug\")\n",
    "\n",
    "#             total_reward += rewards.get('MrX', 0.0)\n",
    "#             state = next_state\n",
    "#             logger.log(f\"Total reward updated to: {total_reward}\",level=\"debug\")\n",
    "#             if done:\n",
    "#                 if winner == 'MrX':\n",
    "#                     wins += 1\n",
    "#                     logger.log(f\"MrX won the evaluation episode.\",level=\"info\")\n",
    "#                 else:\n",
    "#                     logger.log(f\"MrX lost the evaluation episode.\",level=\"info\")\n",
    "\n",
    "#     win_ratio = wins / args.num_eval_episodes\n",
    "#     logger.log(f\"Evaluation completed. Win Ratio: {win_ratio}\")\n",
    "\n",
    "#     logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1}, Total Reward: {total_reward}\",level=\"debug\")\n",
    "\n",
    "#     # win_ratio = evaluate_agent_balance(mrX_agent, police_agent, env, args.num_eval_episodes, device)\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Win Ratio: {win_ratio}\",level=\"info\")\n",
    "\n",
    "#     target_difficulty = compute_target_difficulty(win_ratio)\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Computed target difficulty: {target_difficulty}\",level=\"info\")\n",
    "\n",
    "#     # Train the DifficultyNet based on the difference between predicted and target difficulty\n",
    "#     target_tensor = torch.FloatTensor([target_difficulty]).to(device).requires_grad_()  # Move target to GPU\n",
    "#     win_ratio_tensor = torch.FloatTensor([win_ratio]).to(device).requires_grad_()\n",
    "#     loss = criterion(win_ratio_tensor , target_tensor)\n",
    "#     logger.log(\n",
    "#         f\"Epoch {epoch + 1}: Loss: {loss.item()}, Win Ratio: {win_ratio}, \"\n",
    "#         f\"Real Difficulty: {win_ratio}, Target Difficulty: {target_difficulty}\"\n",
    "#     )\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Optimizer step completed.\",level=\"debug\")\n",
    "\n",
    "#     logger.log_scalar('epoch/loss', loss.item())\n",
    "#     logger.log_scalar('epoch/win_ratio', win_ratio)\n",
    "\n",
    "# logger.log(\"Training completed.\")\n",
    "# logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

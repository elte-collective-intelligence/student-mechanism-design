{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "74fd9902",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d883133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "147497fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "ns = Namespace(\n",
    "    graph_nodes=10,\n",
    "    graph_edges=10,\n",
    "    state_size=1,\n",
    "    action_size=5,\n",
    "    num_episodes=5,\n",
    "    num_eval_episodes=3,\n",
    "    epochs=2,\n",
    "    log_dir=\"./src/experiments/smoke_train/logs\",\n",
    "    wandb_api_key=None,\n",
    "    wandb_project=None,\n",
    "    wandb_entity=None,\n",
    "    wandb_run_name=\"smoke_train\",\n",
    "    wandb_resume=False,\n",
    "    agent_configurations=[\n",
    "        {\"num_police_agents\": 2, \"agent_money\": 16},\n",
    "        {\"num_police_agents\": 2, \"agent_money\": 18},\n",
    "        {\"num_police_agents\": 2, \"agent_money\": 20},\n",
    "        {\"num_police_agents\": 2, \"agent_money\": 22},\n",
    "        {\"num_police_agents\": 2, \"agent_money\": 24},\n",
    "        {\"num_police_agents\": 2, \"agent_money\": 20},\n",
    "        {\"num_police_agents\": 3, \"agent_money\": 10},\n",
    "        {\"num_police_agents\": 3, \"agent_money\": 12},\n",
    "        {\"num_police_agents\": 3, \"agent_money\": 14},\n",
    "        {\"num_police_agents\": 3, \"agent_money\": 16},\n",
    "        {\"num_police_agents\": 3, \"agent_money\": 18},\n",
    "        {\"num_police_agents\": 4, \"agent_money\": 10},\n",
    "        {\"num_police_agents\": 4, \"agent_money\": 12},\n",
    "        {\"num_police_agents\": 4, \"agent_money\": 14},\n",
    "        {\"num_police_agents\": 4, \"agent_money\": 16},\n",
    "        {\"num_police_agents\": 5, \"agent_money\": 8},\n",
    "        {\"num_police_agents\": 5, \"agent_money\": 10},\n",
    "        {\"num_police_agents\": 5, \"agent_money\": 12},\n",
    "        {\"num_police_agents\": 6, \"agent_money\": 10},\n",
    "        {\"num_police_agents\": 6, \"agent_money\": 6},\n",
    "        {\"num_police_agents\": 6, \"agent_money\": 8},\n",
    "    ],\n",
    "    random_seed=42,\n",
    "    evaluate=False,\n",
    "    config=\"./src/experiments/smoke_train/config.yml\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aeb67e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(str(ns).replace(',', '\\n'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "730851c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "from main import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73d408c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = ns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c597136f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from logger import Logger  # Your custom Logger class\n",
    "from RLAgent.gnn_agent import GNNAgent\n",
    "from Enviroment.yard import CustomEnvironment\n",
    "from torch_geometric.data import Data\n",
    "import random\n",
    "from torchrl.envs.libs.pettingzoo import PettingZooWrapper, PettingZooEnv\n",
    "# Define the device at the beginning\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "print(f\"Using device: {device}\")  # You may consider logging this instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "038b92fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 12:40:06,180 - INFO - DifficultyNet initialized and moved to device.\n"
     ]
    }
   ],
   "source": [
    "logger = Logger(\n",
    "        log_dir=args.log_dir,\n",
    "        wandb_api_key=args.wandb_api_key,\n",
    "        wandb_project=args.wandb_project,\n",
    "        wandb_entity=args.wandb_entity,\n",
    "        wandb_run_name=args.wandb_run_name,\n",
    "        wandb_resume=args.wandb_resume\n",
    "    )\n",
    "\n",
    "logger.log(\"Logger initialized.\", level=\"debug\")\n",
    "\n",
    "# Initialize DifficultyNet and move it to the GPU\n",
    "reward_weight_net = RewardWeightNet().to(device)\n",
    "logger.log(\"DifficultyNet initialized and moved to device.\")\n",
    "\n",
    "optimizer = optim.Adam(reward_weight_net.parameters(), lr=0.001)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "logger.log(\"Loss function (MSELoss) initialized.\", level=\"debug\")\n",
    "\n",
    "logger.log(f\"Starting training with variable agents and money settings.\", level=\"debug\")\n",
    "\n",
    "# Validate that the agent configurations list is provided and not empty\n",
    "if not hasattr(args, 'agent_configurations') or not args.agent_configurations:\n",
    "    raise ValueError(\"args.agent_configurations must be a non-empty list of (num_agents, agent_money) tuples.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e16312",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e4c20ec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-26 12:40:54,339 - INFO - Starting epoch 1/2.\n",
      "2025-05-26 12:40:54,342 - INFO - Choosen configuration: 2 agents, 20 money.\n",
      "2025-05-26 12:40:54,548 - INFO - Epoch 1, Episode 1 started.\n",
      "2025-05-26 12:40:54,573 - INFO - Epoch 1, Episode 2 started.\n",
      "2025-05-26 12:40:54,595 - INFO - Epoch 1, Episode 3 started.\n",
      "2025-05-26 12:40:54,626 - INFO - Epoch 1, Episode 4 started.\n",
      "2025-05-26 12:40:54,646 - INFO - Epoch 1, Episode 5 started.\n",
      "2025-05-26 12:40:54,670 - INFO - Starting epoch 2/2.\n",
      "2025-05-26 12:40:54,674 - INFO - Choosen configuration: 3 agents, 18 money.\n",
      "2025-05-26 12:40:54,886 - INFO - Epoch 2, Episode 1 started.\n",
      "2025-05-26 12:40:54,902 - INFO - Epoch 2, Episode 2 started.\n",
      "2025-05-26 12:40:54,915 - INFO - Epoch 2, Episode 3 started.\n",
      "2025-05-26 12:40:54,932 - INFO - Epoch 2, Episode 4 started.\n",
      "2025-05-26 12:40:54,945 - INFO - Epoch 2, Episode 5 started.\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(args.epochs):\n",
    "    logger.log_scalar('epoch_step', epoch)\n",
    "\n",
    "    logger.log(f\"Starting epoch {epoch + 1}/{args.epochs}.\", level=\"info\")\n",
    "    \n",
    "    # Randomly select a (num_agents, agent_money) tuple from the predefined list\n",
    "    # print(args.agent_configurations)\n",
    "    selected_config = random.choice(args.agent_configurations)  # Ensure args.agent_configurations is defined\n",
    "    num_agents, agent_money = selected_config[\"num_police_agents\"], selected_config[\"agent_money\"]  # Unpack the tuple\n",
    "    logger.log(f\"Choosen configuration: {num_agents} agents, {agent_money} money.\", level=\"info\")\n",
    "    # print(selected_config)\n",
    "    logger.log_scalar('epoch/num_agents', num_agents)\n",
    "    logger.log_scalar('epoch/agent_money', agent_money)\n",
    "    # Predict the difficulty from the number of agents and money\n",
    "    inputs = torch.FloatTensor([[num_agents, agent_money, args.graph_nodes, args.graph_edges]]).to(device)  # Move inputs to GPU\n",
    "    predicted_weight = reward_weight_net(inputs)\n",
    "    reward_weights = {\n",
    "        \"Police_distance\" : predicted_weight[0,0],\n",
    "        \"Police_group\": predicted_weight[0,1],\n",
    "        \"Police_position\": predicted_weight[0,2],\n",
    "        \"Police_time\": predicted_weight[0,3],\n",
    "        \"Mrx_closest\": predicted_weight[0,4],\n",
    "        \"Mrx_average\": predicted_weight[0,5],\n",
    "        \"Mrx_position\": predicted_weight[0,6],\n",
    "        \"Mrx_time\": predicted_weight[0,7]\n",
    "    }\n",
    "\n",
    "    logger.log(f\"Epoch {epoch + 1}: Predicted weights: {reward_weights}\", level=\"debug\")\n",
    "    logger.log_weights(reward_weights)\n",
    "    # Create environment with predicted difficulty\n",
    "    env_base = CustomEnvironment(\n",
    "        number_of_agents=num_agents,\n",
    "        agent_money=agent_money,\n",
    "        reward_weights=reward_weights,\n",
    "        logger=logger,\n",
    "        epoch=epoch,\n",
    "        graph_nodes=args.graph_nodes,\n",
    "        graph_edges=args.graph_edges\n",
    "    )\n",
    "    # print(f\"num agents: {num_agents},\\n agent money: {agent_money},\\n reward weights: {reward_weights}\")\n",
    "    # print(f\"graph nodes: {args.graph_nodes},\\n graph edges: {args.graph_edges}\")\n",
    "    env = PettingZooWrapper(env=env_base)\n",
    "    # print(f'observation spec: {env.observation_spec}')\n",
    "    # env = PettingZooEnv(env=env_base, parallel=True)\n",
    "    logger.log(f\"Environment created with weights {reward_weights}.\",level=\"debug\")\n",
    "\n",
    "    # Determine node feature size from the environment\n",
    "    node_feature_size = env.number_of_agents + 1  # Assuming node features exist\n",
    "    mrX_action_size = env.action_space('MrX').n\n",
    "    police_action_size = env.action_space('Police0').n  # Assuming all police have the same action space\n",
    "    logger.log(f\"Node feature size: {node_feature_size}, MrX action size: {mrX_action_size}, Police action size: {police_action_size}\",level=\"debug\")\n",
    "\n",
    "    # Initialize GNN agents with graph-specific parameters and move them to GPU\n",
    "    mrX_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "    police_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "    logger.log(\"GNN agents for MrX and Police initialized.\",level=\"debug\")\n",
    "\n",
    "    # Train the MrX and Police agents in the environment\n",
    "    for episode in range(args.num_episodes):\n",
    "        logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1} started.\",level=\"info\")\n",
    "        # print(episode)\n",
    "        # print(type(env))\n",
    "        # state, _ = env.reset(episode=episode)\n",
    "        state = env.reset(episode=episode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "342636ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = Logger(\n",
    "#         log_dir=args.log_dir,\n",
    "#         wandb_api_key=args.wandb_api_key,\n",
    "#         wandb_project=args.wandb_project,\n",
    "#         wandb_entity=args.wandb_entity,\n",
    "#         wandb_run_name=args.wandb_run_name,\n",
    "#         wandb_resume=args.wandb_resume\n",
    "#     )\n",
    "\n",
    "# logger.log(\"Logger initialized.\", level=\"debug\")\n",
    "\n",
    "# # Initialize DifficultyNet and move it to the GPU\n",
    "# reward_weight_net = RewardWeightNet().to(device)\n",
    "# logger.log(\"DifficultyNet initialized and moved to device.\")\n",
    "\n",
    "# optimizer = optim.Adam(reward_weight_net.parameters(), lr=0.001)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# logger.log(\"Loss function (MSELoss) initialized.\", level=\"debug\")\n",
    "\n",
    "# logger.log(f\"Starting training with variable agents and money settings.\", level=\"debug\")\n",
    "\n",
    "# # Validate that the agent configurations list is provided and not empty\n",
    "# if not hasattr(args, 'agent_configurations') or not args.agent_configurations:\n",
    "#     raise ValueError(\"args.agent_configurations must be a non-empty list of (num_agents, agent_money) tuples.\")\n",
    "\n",
    "# for epoch in range(args.epochs):\n",
    "#     logger.log_scalar('epoch_step', epoch)\n",
    "\n",
    "#     logger.log(f\"Starting epoch {epoch + 1}/{args.epochs}.\", level=\"info\")\n",
    "    \n",
    "#     # Randomly select a (num_agents, agent_money) tuple from the predefined list\n",
    "#     # print(args.agent_configurations)\n",
    "#     selected_config = random.choice(args.agent_configurations)  # Ensure args.agent_configurations is defined\n",
    "#     num_agents, agent_money = selected_config[\"num_police_agents\"], selected_config[\"agent_money\"]  # Unpack the tuple\n",
    "#     logger.log(f\"Choosen configuration: {num_agents} agents, {agent_money} money.\", level=\"info\")\n",
    "#     # print(selected_config)\n",
    "#     logger.log_scalar('epoch/num_agents', num_agents)\n",
    "#     logger.log_scalar('epoch/agent_money', agent_money)\n",
    "#     # Predict the difficulty from the number of agents and money\n",
    "#     inputs = torch.FloatTensor([[num_agents, agent_money, args.graph_nodes, args.graph_edges]]).to(device)  # Move inputs to GPU\n",
    "#     predicted_weight = reward_weight_net(inputs)\n",
    "#     reward_weights = {\n",
    "#         \"Police_distance\" : predicted_weight[0,0],\n",
    "#         \"Police_group\": predicted_weight[0,1],\n",
    "#         \"Police_position\": predicted_weight[0,2],\n",
    "#         \"Police_time\": predicted_weight[0,3],\n",
    "#         \"Mrx_closest\": predicted_weight[0,4],\n",
    "#         \"Mrx_average\": predicted_weight[0,5],\n",
    "#         \"Mrx_position\": predicted_weight[0,6],\n",
    "#         \"Mrx_time\": predicted_weight[0,7]\n",
    "#     }\n",
    "\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Predicted weights: {reward_weights}\", level=\"debug\")\n",
    "#     logger.log_weights(reward_weights)\n",
    "#     # Create environment with predicted difficulty\n",
    "#     env_base = CustomEnvironment(\n",
    "#         number_of_agents=num_agents,\n",
    "#         agent_money=agent_money,\n",
    "#         reward_weights=reward_weights,\n",
    "#         logger=logger,\n",
    "#         epoch=epoch,\n",
    "#         graph_nodes=args.graph_nodes,\n",
    "#         graph_edges=args.graph_edges\n",
    "#     )\n",
    "#     env = PettingZooWrapper(env=env_base)\n",
    "#     logger.log(f\"Environment created with weights {reward_weights}.\",level=\"debug\")\n",
    "\n",
    "#     # Determine node feature size from the environment\n",
    "#     node_feature_size = env.number_of_agents + 1  # Assuming node features exist\n",
    "#     mrX_action_size = env.action_space('MrX').n\n",
    "#     police_action_size = env.action_space('Police0').n  # Assuming all police have the same action space\n",
    "#     logger.log(f\"Node feature size: {node_feature_size}, MrX action size: {mrX_action_size}, Police action size: {police_action_size}\",level=\"debug\")\n",
    "\n",
    "#     # Initialize GNN agents with graph-specific parameters and move them to GPU\n",
    "#     mrX_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "#     police_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "#     logger.log(\"GNN agents for MrX and Police initialized.\",level=\"debug\")\n",
    "\n",
    "#     # Train the MrX and Police agents in the environment\n",
    "#     for episode in range(args.num_episodes):\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1} started.\",level=\"info\")\n",
    "#         state, _ = env.reset(episode=episode)\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             # Create graph data for GNN and move to GPU\n",
    "#             mrX_graph = create_graph_data(state, 'MrX', env).to(device)\n",
    "#             police_graphs = [\n",
    "#                 create_graph_data(state, f'Police{i}', env).to(device)\n",
    "#                 for i in range(num_agents)\n",
    "#             ]\n",
    "#             logger.log(f\"Created graph data for MrX and Police agents.\",level=\"debug\")\n",
    "\n",
    "#             # MrX selects an action\n",
    "#             # mrX_action = mrX_agent.select_action(mrX_graph, torch.ones(mrX_action_size, device=device))\n",
    "\n",
    "#             mrX_action_size = env.action_space('MrX').n\n",
    "#             mrX_possible_moves = env.get_possible_moves(0)\n",
    "#             action_mask = torch.zeros(mrX_graph.num_nodes, dtype=torch.int32, device=device)\n",
    "#             action_mask[ mrX_possible_moves] = 1\n",
    "#             mrX_action = mrX_agent.select_action(mrX_graph,action_mask)\n",
    "#             logger.log(f\"MrX selected action: {mrX_action}\",level=\"debug\")\n",
    "\n",
    "#             # Police agents select actions\n",
    "#             agent_actions = {'MrX': mrX_action}\n",
    "#             for i in range(num_agents):\n",
    "#                 police_action_size = env.action_space(f'Police{i}').n\n",
    "#                 police_possible_moves = env.get_possible_moves(i+1)\n",
    "#                 action_mask = torch.zeros(police_graphs[i].num_nodes, dtype=torch.int32, device=device)\n",
    "#                 action_mask[ police_possible_moves] = 1\n",
    "#                 police_action = police_agent.select_action(\n",
    "#                     police_graphs[i],\n",
    "#                     action_mask\n",
    "#                 )\n",
    "#                 agent_actions[f'Police{i}'] = police_action\n",
    "#                 logger.log(f\"Police{i} selected action: {police_action}\",level=\"debug\")\n",
    "\n",
    "#             # Execute actions for MrX and Police\n",
    "#             next_state, rewards, terminations, truncation, _, _ = env.step(agent_actions)\n",
    "#             logger.log(f\"Executed actions. Rewards: {rewards}, Terminations: {terminations}, Truncations: {truncation}\",level=\"debug\")\n",
    "\n",
    "#             done = terminations.get('Police0', False) or all(truncation.values())\n",
    "#             logger.log(f\"Episode done: {done}\",level=\"debug\")\n",
    "\n",
    "#             # Update MrX agent\n",
    "#             mrX_next_graph = create_graph_data(next_state, 'MrX', env).to(device)\n",
    "#             mrX_agent.update(\n",
    "#                 mrX_graph,\n",
    "#                 mrX_action,\n",
    "#                 rewards.get('MrX', 0.0),\n",
    "#                 mrX_next_graph,\n",
    "#                 not terminations.get('Police0', False)\n",
    "#             )\n",
    "#             logger.log(f\"MrX agent updated with reward: {rewards.get('MrX', 0.0)}\",level=\"debug\")\n",
    "\n",
    "#             # Update shared police agent\n",
    "#             for i in range(num_agents):\n",
    "#                 police_next_graph = create_graph_data(next_state, f'Police{i}', env).to(device)\n",
    "#                 police_agent.update(\n",
    "#                     police_graphs[i],\n",
    "#                     agent_actions.get(f'Police{i}'),\n",
    "#                     rewards.get(f'Police{i}', 0.0),\n",
    "#                     police_next_graph,\n",
    "#                     terminations.get(f'Police{i}', False)\n",
    "#                 )\n",
    "#                 logger.log(f\"Police{i} agent updated with reward: {rewards.get(f'Police{i}', 0.0)}\",level=\"debug\")\n",
    "\n",
    "#             total_reward += rewards.get('MrX', 0.0)\n",
    "#             state = next_state\n",
    "#             logger.log(f\"Total reward updated to: {total_reward}\",level=\"debug\")\n",
    "\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1}, Total Reward: {total_reward}\",level=\"debug\")\n",
    "#         # logger.log_scalar(f'Episode_total_reward{epoch}', total_reward, episode)\n",
    "\n",
    "#     # Evaluate performance and calculate the target difficulty\n",
    "#     logger.log(f\"Evaluating agent balance after epoch {epoch + 1}.\",level=\"debug\")\n",
    "#     # logger.log_model(mrX_agent, 'MrX')\n",
    "#     # logger.log_model(police_agent, 'Police')\n",
    "#     # logger.log_model(reward_weight_net, 'RewardWeightNet')\n",
    "\n",
    "#     wins = 0\n",
    "\n",
    "#     for episode in range(args.num_eval_episodes):\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Evaluation Episode {episode + 1} started.\",level=\"info\")\n",
    "#         state, _ = env.reset(episode=episode)\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             # Create graph data for GNN and move to GPU\n",
    "#             mrX_graph = create_graph_data(state, 'MrX', env).to(device)\n",
    "#             police_graphs = [\n",
    "#                 create_graph_data(state, f'Police{i}', env).to(device)\n",
    "#                 for i in range(num_agents)\n",
    "#             ]\n",
    "#             logger.log(f\"Created graph data for MrX and Police agents.\",level=\"debug\")\n",
    "\n",
    "#             # MrX selects an action\n",
    "#             # mrX_action = mrX_agent.select_action(mrX_graph, torch.ones(mrX_action_size, device=device))\n",
    "\n",
    "#             mrX_action_size = env.action_space('MrX').n\n",
    "#             mrX_possible_moves = env.get_possible_moves(0)\n",
    "#             action_mask = torch.zeros(mrX_graph.num_nodes, dtype=torch.int32, device=device)\n",
    "#             action_mask[ mrX_possible_moves] = 1\n",
    "#             mrX_action = mrX_agent.select_action(mrX_graph,action_mask)\n",
    "#             logger.log(f\"MrX selected action: {mrX_action}\",level=\"debug\")\n",
    "\n",
    "#             # Police agents select actions\n",
    "#             agent_actions = {'MrX': mrX_action}\n",
    "#             for i in range(num_agents):\n",
    "#                 police_action_size = env.action_space(f'Police{i}').n\n",
    "#                 police_possible_moves = env.get_possible_moves(i+1)\n",
    "#                 action_mask = torch.zeros(police_graphs[i].num_nodes, dtype=torch.int32, device=device)\n",
    "#                 action_mask[ police_possible_moves] = 1\n",
    "#                 police_action = police_agent.select_action(\n",
    "#                     police_graphs[i],\n",
    "#                     action_mask\n",
    "#                 )\n",
    "#                 agent_actions[f'Police{i}'] = police_action\n",
    "#                 logger.log(f\"Police{i} selected action: {police_action}\",level=\"debug\")\n",
    "\n",
    "#             # Execute actions for MrX and Police\n",
    "#             next_state, rewards, terminations, truncation, winner, _ = env.step(agent_actions)\n",
    "#             logger.log(f\"Executed actions. Rewards: {rewards}, Terminations: {terminations}, Truncations: {truncation}\",level=\"debug\")\n",
    "\n",
    "#             done = terminations.get('Police0', False) or all(truncation.values())\n",
    "#             logger.log(f\"Episode done: {done}\",level=\"debug\")\n",
    "\n",
    "#             total_reward += rewards.get('MrX', 0.0)\n",
    "#             state = next_state\n",
    "#             logger.log(f\"Total reward updated to: {total_reward}\",level=\"debug\")\n",
    "#             if done:\n",
    "#                 if winner == 'MrX':\n",
    "#                     wins += 1\n",
    "#                     logger.log(f\"MrX won the evaluation episode.\",level=\"info\")\n",
    "#                 else:\n",
    "#                     logger.log(f\"MrX lost the evaluation episode.\",level=\"info\")\n",
    "\n",
    "#     win_ratio = wins / args.num_eval_episodes\n",
    "#     logger.log(f\"Evaluation completed. Win Ratio: {win_ratio}\")\n",
    "\n",
    "#     logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1}, Total Reward: {total_reward}\",level=\"debug\")\n",
    "\n",
    "#     # win_ratio = evaluate_agent_balance(mrX_agent, police_agent, env, args.num_eval_episodes, device)\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Win Ratio: {win_ratio}\",level=\"info\")\n",
    "\n",
    "#     target_difficulty = compute_target_difficulty(win_ratio)\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Computed target difficulty: {target_difficulty}\",level=\"info\")\n",
    "\n",
    "#     # Train the DifficultyNet based on the difference between predicted and target difficulty\n",
    "#     target_tensor = torch.FloatTensor([target_difficulty]).to(device).requires_grad_()  # Move target to GPU\n",
    "#     win_ratio_tensor = torch.FloatTensor([win_ratio]).to(device).requires_grad_()\n",
    "#     loss = criterion(win_ratio_tensor , target_tensor)\n",
    "#     logger.log(\n",
    "#         f\"Epoch {epoch + 1}: Loss: {loss.item()}, Win Ratio: {win_ratio}, \"\n",
    "#         f\"Real Difficulty: {win_ratio}, Target Difficulty: {target_difficulty}\"\n",
    "#     )\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Optimizer step completed.\",level=\"debug\")\n",
    "\n",
    "#     logger.log_scalar('epoch/loss', loss.item())\n",
    "#     logger.log_scalar('epoch/win_ratio', win_ratio)\n",
    "\n",
    "# logger.log(\"Training completed.\")\n",
    "# logger.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2044a39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# logger = Logger(\n",
    "#         log_dir=args.log_dir,\n",
    "#         wandb_api_key=args.wandb_api_key,\n",
    "#         wandb_project=args.wandb_project,\n",
    "#         wandb_entity=args.wandb_entity,\n",
    "#         wandb_run_name=args.wandb_run_name,\n",
    "#         wandb_resume=args.wandb_resume\n",
    "#     )\n",
    "\n",
    "# logger.log(\"Logger initialized.\", level=\"debug\")\n",
    "\n",
    "# # Initialize DifficultyNet and move it to the GPU\n",
    "# reward_weight_net = RewardWeightNet().to(device)\n",
    "# logger.log(\"DifficultyNet initialized and moved to device.\")\n",
    "\n",
    "# optimizer = optim.Adam(reward_weight_net.parameters(), lr=0.001)\n",
    "\n",
    "# criterion = nn.MSELoss()\n",
    "# logger.log(\"Loss function (MSELoss) initialized.\", level=\"debug\")\n",
    "\n",
    "# logger.log(f\"Starting training with variable agents and money settings.\", level=\"debug\")\n",
    "\n",
    "# # Validate that the agent configurations list is provided and not empty\n",
    "# if not hasattr(args, 'agent_configurations') or not args.agent_configurations:\n",
    "#     raise ValueError(\"args.agent_configurations must be a non-empty list of (num_agents, agent_money) tuples.\")\n",
    "\n",
    "# for epoch in range(args.epochs):\n",
    "#     logger.log_scalar('epoch_step', epoch)\n",
    "\n",
    "#     logger.log(f\"Starting epoch {epoch + 1}/{args.epochs}.\", level=\"info\")\n",
    "    \n",
    "#     # Randomly select a (num_agents, agent_money) tuple from the predefined list\n",
    "#     # print(args.agent_configurations)\n",
    "#     selected_config = random.choice(args.agent_configurations)  # Ensure args.agent_configurations is defined\n",
    "#     num_agents, agent_money = selected_config[\"num_police_agents\"], selected_config[\"agent_money\"]  # Unpack the tuple\n",
    "#     logger.log(f\"Choosen configuration: {num_agents} agents, {agent_money} money.\", level=\"info\")\n",
    "#     # print(selected_config)\n",
    "#     logger.log_scalar('epoch/num_agents', num_agents)\n",
    "#     logger.log_scalar('epoch/agent_money', agent_money)\n",
    "#     # Predict the difficulty from the number of agents and money\n",
    "#     inputs = torch.FloatTensor([[num_agents, agent_money, args.graph_nodes, args.graph_edges]]).to(device)  # Move inputs to GPU\n",
    "#     predicted_weight = reward_weight_net(inputs)\n",
    "#     reward_weights = {\n",
    "#         \"Police_distance\" : predicted_weight[0,0],\n",
    "#         \"Police_group\": predicted_weight[0,1],\n",
    "#         \"Police_position\": predicted_weight[0,2],\n",
    "#         \"Police_time\": predicted_weight[0,3],\n",
    "#         \"Mrx_closest\": predicted_weight[0,4],\n",
    "#         \"Mrx_average\": predicted_weight[0,5],\n",
    "#         \"Mrx_position\": predicted_weight[0,6],\n",
    "#         \"Mrx_time\": predicted_weight[0,7]\n",
    "#     }\n",
    "\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Predicted weights: {reward_weights}\", level=\"debug\")\n",
    "#     logger.log_weights(reward_weights)\n",
    "#     # Create environment with predicted difficulty\n",
    "#     env_base = CustomEnvironment(\n",
    "#         number_of_agents=num_agents,\n",
    "#         agent_money=agent_money,\n",
    "#         reward_weights=reward_weights,\n",
    "#         logger=logger,\n",
    "#         epoch=epoch,\n",
    "#         graph_nodes=args.graph_nodes,\n",
    "#         graph_edges=args.graph_edges\n",
    "#     )\n",
    "#     env = PettingZooWrapper(env=env_base)\n",
    "#     logger.log(f\"Environment created with weights {reward_weights}.\",level=\"debug\")\n",
    "\n",
    "#     # Determine node feature size from the environment\n",
    "#     node_feature_size = env.number_of_agents + 1  # Assuming node features exist\n",
    "#     mrX_action_size = env.action_space('MrX').n\n",
    "#     police_action_size = env.action_space('Police0').n  # Assuming all police have the same action space\n",
    "#     logger.log(f\"Node feature size: {node_feature_size}, MrX action size: {mrX_action_size}, Police action size: {police_action_size}\",level=\"debug\")\n",
    "\n",
    "#     # Initialize GNN agents with graph-specific parameters and move them to GPU\n",
    "#     mrX_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "#     police_agent = GNNAgent(node_feature_size=node_feature_size, device=device)\n",
    "#     logger.log(\"GNN agents for MrX and Police initialized.\",level=\"debug\")\n",
    "\n",
    "#     # Train the MrX and Police agents in the environment\n",
    "#     for episode in range(args.num_episodes):\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1} started.\",level=\"info\")\n",
    "#         state, _ = env.reset(episode=episode)\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             # Create graph data for GNN and move to GPU\n",
    "#             mrX_graph = create_graph_data(state, 'MrX', env).to(device)\n",
    "#             police_graphs = [\n",
    "#                 create_graph_data(state, f'Police{i}', env).to(device)\n",
    "#                 for i in range(num_agents)\n",
    "#             ]\n",
    "#             logger.log(f\"Created graph data for MrX and Police agents.\",level=\"debug\")\n",
    "\n",
    "#             # MrX selects an action\n",
    "#             # mrX_action = mrX_agent.select_action(mrX_graph, torch.ones(mrX_action_size, device=device))\n",
    "\n",
    "#             mrX_action_size = env.action_space('MrX').n\n",
    "#             mrX_possible_moves = env.get_possible_moves(0)\n",
    "#             action_mask = torch.zeros(mrX_graph.num_nodes, dtype=torch.int32, device=device)\n",
    "#             action_mask[ mrX_possible_moves] = 1\n",
    "#             mrX_action = mrX_agent.select_action(mrX_graph,action_mask)\n",
    "#             logger.log(f\"MrX selected action: {mrX_action}\",level=\"debug\")\n",
    "\n",
    "#             # Police agents select actions\n",
    "#             agent_actions = {'MrX': mrX_action}\n",
    "#             for i in range(num_agents):\n",
    "#                 police_action_size = env.action_space(f'Police{i}').n\n",
    "#                 police_possible_moves = env.get_possible_moves(i+1)\n",
    "#                 action_mask = torch.zeros(police_graphs[i].num_nodes, dtype=torch.int32, device=device)\n",
    "#                 action_mask[ police_possible_moves] = 1\n",
    "#                 police_action = police_agent.select_action(\n",
    "#                     police_graphs[i],\n",
    "#                     action_mask\n",
    "#                 )\n",
    "#                 agent_actions[f'Police{i}'] = police_action\n",
    "#                 logger.log(f\"Police{i} selected action: {police_action}\",level=\"debug\")\n",
    "\n",
    "#             # Execute actions for MrX and Police\n",
    "#             next_state, rewards, terminations, truncation, _, _ = env.step(agent_actions)\n",
    "#             logger.log(f\"Executed actions. Rewards: {rewards}, Terminations: {terminations}, Truncations: {truncation}\",level=\"debug\")\n",
    "\n",
    "#             done = terminations.get('Police0', False) or all(truncation.values())\n",
    "#             logger.log(f\"Episode done: {done}\",level=\"debug\")\n",
    "\n",
    "#             # Update MrX agent\n",
    "#             mrX_next_graph = create_graph_data(next_state, 'MrX', env).to(device)\n",
    "#             mrX_agent.update(\n",
    "#                 mrX_graph,\n",
    "#                 mrX_action,\n",
    "#                 rewards.get('MrX', 0.0),\n",
    "#                 mrX_next_graph,\n",
    "#                 not terminations.get('Police0', False)\n",
    "#             )\n",
    "#             logger.log(f\"MrX agent updated with reward: {rewards.get('MrX', 0.0)}\",level=\"debug\")\n",
    "\n",
    "#             # Update shared police agent\n",
    "#             for i in range(num_agents):\n",
    "#                 police_next_graph = create_graph_data(next_state, f'Police{i}', env).to(device)\n",
    "#                 police_agent.update(\n",
    "#                     police_graphs[i],\n",
    "#                     agent_actions.get(f'Police{i}'),\n",
    "#                     rewards.get(f'Police{i}', 0.0),\n",
    "#                     police_next_graph,\n",
    "#                     terminations.get(f'Police{i}', False)\n",
    "#                 )\n",
    "#                 logger.log(f\"Police{i} agent updated with reward: {rewards.get(f'Police{i}', 0.0)}\",level=\"debug\")\n",
    "\n",
    "#             total_reward += rewards.get('MrX', 0.0)\n",
    "#             state = next_state\n",
    "#             logger.log(f\"Total reward updated to: {total_reward}\",level=\"debug\")\n",
    "\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1}, Total Reward: {total_reward}\",level=\"debug\")\n",
    "#         # logger.log_scalar(f'Episode_total_reward{epoch}', total_reward, episode)\n",
    "\n",
    "#     # Evaluate performance and calculate the target difficulty\n",
    "#     logger.log(f\"Evaluating agent balance after epoch {epoch + 1}.\",level=\"debug\")\n",
    "#     # logger.log_model(mrX_agent, 'MrX')\n",
    "#     # logger.log_model(police_agent, 'Police')\n",
    "#     # logger.log_model(reward_weight_net, 'RewardWeightNet')\n",
    "\n",
    "#     wins = 0\n",
    "\n",
    "#     for episode in range(args.num_eval_episodes):\n",
    "#         logger.log(f\"Epoch {epoch + 1}, Evaluation Episode {episode + 1} started.\",level=\"info\")\n",
    "#         state, _ = env.reset(episode=episode)\n",
    "#         done = False\n",
    "#         total_reward = 0\n",
    "\n",
    "#         while not done:\n",
    "#             # Create graph data for GNN and move to GPU\n",
    "#             mrX_graph = create_graph_data(state, 'MrX', env).to(device)\n",
    "#             police_graphs = [\n",
    "#                 create_graph_data(state, f'Police{i}', env).to(device)\n",
    "#                 for i in range(num_agents)\n",
    "#             ]\n",
    "#             logger.log(f\"Created graph data for MrX and Police agents.\",level=\"debug\")\n",
    "\n",
    "#             # MrX selects an action\n",
    "#             # mrX_action = mrX_agent.select_action(mrX_graph, torch.ones(mrX_action_size, device=device))\n",
    "\n",
    "#             mrX_action_size = env.action_space('MrX').n\n",
    "#             mrX_possible_moves = env.get_possible_moves(0)\n",
    "#             action_mask = torch.zeros(mrX_graph.num_nodes, dtype=torch.int32, device=device)\n",
    "#             action_mask[ mrX_possible_moves] = 1\n",
    "#             mrX_action = mrX_agent.select_action(mrX_graph,action_mask)\n",
    "#             logger.log(f\"MrX selected action: {mrX_action}\",level=\"debug\")\n",
    "\n",
    "#             # Police agents select actions\n",
    "#             agent_actions = {'MrX': mrX_action}\n",
    "#             for i in range(num_agents):\n",
    "#                 police_action_size = env.action_space(f'Police{i}').n\n",
    "#                 police_possible_moves = env.get_possible_moves(i+1)\n",
    "#                 action_mask = torch.zeros(police_graphs[i].num_nodes, dtype=torch.int32, device=device)\n",
    "#                 action_mask[ police_possible_moves] = 1\n",
    "#                 police_action = police_agent.select_action(\n",
    "#                     police_graphs[i],\n",
    "#                     action_mask\n",
    "#                 )\n",
    "#                 agent_actions[f'Police{i}'] = police_action\n",
    "#                 logger.log(f\"Police{i} selected action: {police_action}\",level=\"debug\")\n",
    "\n",
    "#             # Execute actions for MrX and Police\n",
    "#             next_state, rewards, terminations, truncation, winner, _ = env.step(agent_actions)\n",
    "#             logger.log(f\"Executed actions. Rewards: {rewards}, Terminations: {terminations}, Truncations: {truncation}\",level=\"debug\")\n",
    "\n",
    "#             done = terminations.get('Police0', False) or all(truncation.values())\n",
    "#             logger.log(f\"Episode done: {done}\",level=\"debug\")\n",
    "\n",
    "#             total_reward += rewards.get('MrX', 0.0)\n",
    "#             state = next_state\n",
    "#             logger.log(f\"Total reward updated to: {total_reward}\",level=\"debug\")\n",
    "#             if done:\n",
    "#                 if winner == 'MrX':\n",
    "#                     wins += 1\n",
    "#                     logger.log(f\"MrX won the evaluation episode.\",level=\"info\")\n",
    "#                 else:\n",
    "#                     logger.log(f\"MrX lost the evaluation episode.\",level=\"info\")\n",
    "\n",
    "#     win_ratio = wins / args.num_eval_episodes\n",
    "#     logger.log(f\"Evaluation completed. Win Ratio: {win_ratio}\")\n",
    "\n",
    "#     logger.log(f\"Epoch {epoch + 1}, Episode {episode + 1}, Total Reward: {total_reward}\",level=\"debug\")\n",
    "\n",
    "#     # win_ratio = evaluate_agent_balance(mrX_agent, police_agent, env, args.num_eval_episodes, device)\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Win Ratio: {win_ratio}\",level=\"info\")\n",
    "\n",
    "#     target_difficulty = compute_target_difficulty(win_ratio)\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Computed target difficulty: {target_difficulty}\",level=\"info\")\n",
    "\n",
    "#     # Train the DifficultyNet based on the difference between predicted and target difficulty\n",
    "#     target_tensor = torch.FloatTensor([target_difficulty]).to(device).requires_grad_()  # Move target to GPU\n",
    "#     win_ratio_tensor = torch.FloatTensor([win_ratio]).to(device).requires_grad_()\n",
    "#     loss = criterion(win_ratio_tensor , target_tensor)\n",
    "#     logger.log(\n",
    "#         f\"Epoch {epoch + 1}: Loss: {loss.item()}, Win Ratio: {win_ratio}, \"\n",
    "#         f\"Real Difficulty: {win_ratio}, Target Difficulty: {target_difficulty}\"\n",
    "#     )\n",
    "#     optimizer.zero_grad()\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "#     logger.log(f\"Epoch {epoch + 1}: Optimizer step completed.\",level=\"debug\")\n",
    "\n",
    "#     logger.log_scalar('epoch/loss', loss.item())\n",
    "#     logger.log_scalar('epoch/win_ratio', win_ratio)\n",
    "\n",
    "# logger.log(\"Training completed.\")\n",
    "# logger.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ci",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
